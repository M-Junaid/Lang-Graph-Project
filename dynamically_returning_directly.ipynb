{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        " we'll build a custom chat executor that allows a Language Learning Model (LLM) to dynamically decide whether to return the result of a tool call as the final answer or to process it further before responding. This approach is particularly useful when you have tools that can generate acceptable final answers on their own, and you want the LLM to intelligently determine when this is the case.\n",
        "\n",
        "The example builds upon the base chat executor, so it's recommended to familiarize yourself with that before proceeding. We'll be creating the agent from scratch for transparency, though similar functionality can be achieved using the create_react_agent constructor provided by LangChain.\n",
        "\n",
        "By following this guide, you'll learn how to:\n",
        "\n",
        "1.  Set up the necessary tools and APIs.\n",
        "2. Define a custom tool with additional parameters.\n",
        "3. Implement an agent state and a state graph to manage the conversation flow.\n",
        "4. Build a chat agent that can decide when to finalize a response based on the  output of tool invocations.\n",
        "\n",
        "This hands-on example will give you a deeper understanding of how to build flexible and context-aware chat agents using LangChain and LangGraph."
      ],
      "metadata": {
        "id": "R7nuVugToht7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Installation"
      ],
      "metadata": {
        "id": "7Q_fm2b8wlSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain langchain_openai tavily-python"
      ],
      "metadata": {
        "id": "LVL_qSl6wgaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Explanation\n",
        "* Purpose: This block installs the necessary Python packages:\n",
        "  * langgraph: For creating and managing graphs in LangChain.\n",
        "  * langchain: For working with LLMs and their tools.\n",
        "  * langchain_openai: Specifically for integrating OpenAI models.\n",
        "tavily-python: For using the Tavily search tool."
      ],
      "metadata": {
        "id": "Sl5DJC0rwrNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n"
      ],
      "metadata": {
        "id": "5XvY4o5AwgSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Purpose: This block sets up the environment variables for the OpenAI and Tavily API keys. These keys are essential for authenticating your requests to these services."
      ],
      "metadata": {
        "id": "sTDFuBm1xSAu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5b3GunPwgIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting Up the Tools\n",
        "We will first define the tools we want to use. For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation here on how to do that."
      ],
      "metadata": {
        "id": "QZOVlSlTxZzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field  # Import BaseModel and Field from Pydantic, used for data validation and model creation\n",
        "\n",
        "# Define a class called SearchTool that inherits from BaseModel\n",
        "class SearchTool(BaseModel):\n",
        "    \"\"\"A model representing an online search tool, with an option for direct result return.\"\"\"\n",
        "\n",
        "    # Define a field 'query' which is a string, representing the search query\n",
        "    query: str = Field(description=\"The search query to look up online\")\n",
        "\n",
        "    # Define a field 'return_direct' which is a boolean, determining whether the result is returned directly\n",
        "    return_direct: bool = Field(\n",
        "        description=\"Indicates if the result should be returned directly to the user without processing or viewing by the agent\",\n",
        "        default=False,  # Set the default value to False, meaning the result is not returned directly unless specified\n",
        "    )\n"
      ],
      "metadata": {
        "id": "VbiSxcZVwf9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Explanation\n",
        "The SearchTool class is a data model used to define the structure for an online search tool. It has two attributes:\n",
        "* query: A string representing the search query.\n",
        "* return_direct: A boolean indicating whether the search result should be returned directly to the user without any additional processing or oversight.\n",
        "\n",
        "\n",
        "The BaseModel from Pydantic ensures that these fields are validated and serialized correctly, making it easier to work with structured data in Python."
      ],
      "metadata": {
        "id": "Eo4ZbdiMxfmu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8BfE3YgfAeqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL_jK6JAeaQD",
        "outputId": "e38e63ac-5608-442d-aaa2-20f54949cbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.15)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.35 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.36)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.106)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults  # Import TavilySearchResults from the langchain_community.tools.tavily_search module\n",
        "\n",
        "# Create an instance of TavilySearchResults with specific configurations\n",
        "search_tool = TavilySearchResults(\n",
        "    max_results=1,  # Limit the search results to a maximum of 1 result\n",
        "    args_schema=SearchTool  # Use the SearchTool class to define the schema for search arguments\n",
        ")\n",
        "\n",
        "# Create a list containing the search_tool instance\n",
        "tools = [search_tool]  # This list can be used to manage and access different tools\n"
      ],
      "metadata": {
        "id": "DlpoUOIkyNqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " This code snippet sets up a search tool using the Tavily search engine or API, configures it to return a maximum of one result, and ensures that the search query adheres to the structure defined by the SearchTool class.\n",
        "\n",
        "* TavilySearchResults: Represents the search tool.\n",
        "* max_results=1: Limits the search to one result.\n",
        "* args_schema=SearchTool: Ensures that the input follows the SearchTool schema.\n",
        "* tools = [search_tool]: Prepares a list containing the search_tool, which can be passed into other parts of your code, such as an agent that uses this tool to perform searches."
      ],
      "metadata": {
        "id": "j_9SbMj8ylyb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wa7OtMJlyNh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Tool Executor\n",
        "We can now wrap these tools in a simple ToolExecutor. This is a real simple class that takes in a ToolInvocation and calls that tool, returning the output. A ToolInvocation is any class with tool and tool_input attribute."
      ],
      "metadata": {
        "id": "d6w2-sMyyp3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolExecutor  # Import ToolExecutor from langgraph.prebuilt to manage and execute tools\n",
        "\n",
        "tool_executor = ToolExecutor(tools)  # Create an instance of ToolExecutor, providing the list of tools to be managed and executed\n"
      ],
      "metadata": {
        "id": "7DwY08aGyNeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0046f98d-79f2-4567-f89c-47f4ab88b991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-a7d6763a57d7>:3: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
            "  tool_executor = ToolExecutor(tools)  # Create an instance of ToolExecutor, providing the list of tools to be managed and executed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Purpose: This creates a ToolExecutor instance that can execute the tools defined earlier. The ToolExecutor is responsible for taking a ToolInvocation and returning the output."
      ],
      "metadata": {
        "id": "kaVsvv1QyyB2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LHlDK_6yNZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Setting Up the Model\n",
        "Now we need to load the chat model we want to use. Importantly, this should satisfy two criteria:\n",
        "\n",
        "1. It should work with messages. We will represent all agent state in the form of messages, so it needs to be able to work well with them.\n",
        "2. It should work with OpenAI function calling. This means it should either be an OpenAI model or a model that exposes a similar interface."
      ],
      "metadata": {
        "id": "YSaxLV5Cy6jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI  # Import ChatOpenAI to use OpenAI's chat model\n",
        "\n",
        "# Create an instance of ChatOpenAI with specific settings\n",
        "model = ChatOpenAI(\n",
        "    temperature=0,  # Set temperature to 0 for more deterministic responses (less randomness)\n",
        "    streaming=True  # Enable streaming to receive responses in real-time as they are generated\n",
        ")\n",
        "\n",
        "# Bind the previously defined tools to the model, so it can use them during its operation\n",
        "model = model.bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "8bZCm2jhyNIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "* ChatOpenAI: This loads the OpenAI chat model that will handle the conversation. The temperature=0 makes the model deterministic (it won't generate random responses).\n",
        "* bind_tools: Associates the defined tools with the model so it knows these tools are available for use. binds the tools (like the Tavily search tool) to the model, allowing the model to use these tools during conversation."
      ],
      "metadata": {
        "id": "k1ccAYdAzDNN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ENHGSJJAyM89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Defining Agent State\n",
        "In LangGraph, when we talk about the \"agent state,\" we're referring to a piece of data that keeps track of what's happening as the agent moves through different parts of the graph.\n",
        "\n",
        "# Breaking it Down:\n",
        "**StateGraph:**\n",
        "\n",
        "* This is the main structure in LangGraph that controls how the agent moves through various steps (or nodes) in the graph.\n",
        "\n",
        "**State Object:**\n",
        "\n",
        "* This is like a notebook that the StateGraph carries around. It holds information that might change as the agent goes from one node to another.\n",
        "\n",
        "**Nodes and State Updates:**\n",
        "\n",
        "* Each node in the graph does something specific. When the agent reaches a node, that node can either:\n",
        "\n",
        "**SET:** Change something in the state notebook (like replacing an old note with a new one).\n",
        "**ADD:** Add something new to the state notebook without changing what's already there.\n",
        "\n",
        "**Example with Messages:**\n",
        "\n",
        "* Imagine the state notebook is just a list where you jot down messages. As the agent moves through different nodes, each node adds a new message to this list.\n",
        "* We use something called a TypedDict to define what this notebook looks like. In this case, it has one key, messages, which is just a list where new messages are added as the agent moves along.\n",
        "\n",
        "So, the \"agent state\" is basically a way to track what's happening as the agent progresses through different tasks, with each step either adding to or changing the information being tracked."
      ],
      "metadata": {
        "id": "wGtUNMS_zd9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import operator  # Import the operator module for functions that can be used with operators\n",
        "\n",
        "from typing import Annotated, Sequence, TypedDict  # Import types for type hinting and type definitions\n",
        "\n",
        "from langchain_core.messages import BaseMessage  # Import BaseMessage to represent messages in the workflow\n",
        "\n",
        "# Define a class called AgentState that specifies the structure of the state dictionary\n",
        "class AgentState(TypedDict):\n",
        "    # Define a field 'messages' which is a sequence (list) of BaseMessage objects\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]  # Annotate with operator.add for special handling or metadata\n"
      ],
      "metadata": {
        "id": "QBmG6Es5zb1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "AgentState: This defines the structure of the state that will be passed around between different nodes (components) in the workflow. It contains a list of messages that represent the conversation history."
      ],
      "metadata": {
        "id": "xvsrHfEvzoRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMTugIwYzbwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Defining the Nodes\n",
        "We now need to define a few different nodes in our graph. In langgraph, a node can be either a function or a runnable. There are two main nodes we need for this:\n",
        "\n",
        "1. The agent: responsible for deciding what (if any) actions to take.\n",
        "2. A function to invoke tools: if the agent decides to take an action, this node will then execute that action.\n",
        "\n",
        "We will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides).\n",
        "\n",
        "1. Conditional Edge: after the agent is called, we should either: a. If the agent said to take an action, then the function to invoke tools should be called b. If the agent said that it was finished, then it should finish\n",
        "2. Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next\n",
        "\n",
        "Let's define the nodes, as well as a function to decide how what conditional edge to take.\n",
        "\n",
        "**MODIFICATION**\n",
        "\n",
        "We change the should_continue function to check whether return_direct was set to True"
      ],
      "metadata": {
        "id": "ctgDL2dtz_8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import ToolMessage  # Import ToolMessage for handling messages related to tools\n",
        "from langgraph.prebuilt import ToolInvocation  # Import ToolInvocation to create and manage tool calls\n",
        "\n",
        "# Define a function to decide whether to continue processing based on the state\n",
        "def should_continue(state):\n",
        "    messages = state[\"messages\"]  # Get the list of messages from the state\n",
        "    last_message = messages[-1]  # Get the most recent message\n",
        "\n",
        "    # Check if the last message has any tool calls\n",
        "    if not last_message.tool_calls:\n",
        "        return \"end\"  # If no tool calls, return \"end\" to stop processing\n",
        "    else:\n",
        "        # Get the arguments of the first tool call in the last message\n",
        "        arguments = last_message.tool_calls[0][\"args\"]\n",
        "\n",
        "        # Check if the \"return_direct\" argument is set to True\n",
        "        if arguments.get(\"return_direct\", False):\n",
        "            return \"final\"  # If \"return_direct\" is True, return \"final\" to process the result directly\n",
        "        else:\n",
        "            return \"continue\"  # If \"return_direct\" is not True, return \"continue\" to proceed with further processing\n"
      ],
      "metadata": {
        "id": "D6Q3NHTgzbqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "should_continue: Decides whether to continue processing or to end the conversation. It checks if the tool's response should be returned directly (return_direct=True), and based on that, decides the next step."
      ],
      "metadata": {
        "id": "nY41FaFK5nFO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_SMexcZZXg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state):  # Define a function named 'call_model' that takes one argument 'state'\n",
        "    messages = state[\"messages\"]  # Extract the value associated with the key 'messages' from the 'state' dictionary and store it in the variable 'messages'\n",
        "    response = model.invoke(messages)  # Use the 'invoke' method of 'model' to process 'messages' and store the result in the variable 'response'\n",
        "    return {\"messages\": [response]}  # Return a dictionary with the key 'messages' and the value as a list containing 'response'\n"
      ],
      "metadata": {
        "id": "xCLpk7h_5sc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* call_model: Calls the language model with the current conversation history and appends the model’s response to the message list."
      ],
      "metadata": {
        "id": "01Z9Q1tw5zvV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "votL4RxR54Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODIFICATION**\n",
        "\n",
        "We change the tool calling to get rid of the return_direct parameter (not used in the actual tool call)"
      ],
      "metadata": {
        "id": "q90vbcK-cQ6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_tool(state):  # Define a function named 'call_tool' that takes one argument 'state'\n",
        "    messages = state[\"messages\"]  # Extract the 'messages' list from the 'state' dictionary\n",
        "    last_message = messages[-1]  # Get the last message in the 'messages' list\n",
        "    tool_call = last_message.tool_calls[0]  # Extract the first tool call from the 'tool_calls' list in the last message\n",
        "    tool_name = tool_call[\"name\"]  # Get the name of the tool from the 'tool_call' dictionary\n",
        "    arguments = tool_call[\"args\"]  # Get the arguments for the tool from the 'tool_call' dictionary\n",
        "\n",
        "    if tool_name == \"tavily_search_results_json\":  # Check if the tool name is \"tavily_search_results_json\"\n",
        "        if \"return_direct\" in arguments:  # Check if the argument \"return_direct\" exists in the 'arguments' dictionary\n",
        "            del arguments[\"return_direct\"]  # If it exists, delete the \"return_direct\" key from the 'arguments' dictionary\n",
        "\n",
        "    action = ToolInvocation(tool=tool_name, tool_input=arguments)  # Create an action using 'ToolInvocation' with the tool's name and the modified arguments\n",
        "    response = tool_executor.invoke(action)  # Invoke the tool action using 'tool_executor' and store the response\n",
        "\n",
        "    tool_message = ToolMessage(content=str(response), name=action.tool, tool_call_id=tool_call[\"id\"])  # Create a new 'ToolMessage' with the response content, tool name, and tool call ID\n",
        "    return {\"messages\": [tool_message]}  # Return a dictionary containing the new 'tool_message' in a list under the key 'messages'\n"
      ],
      "metadata": {
        "id": "I8-bRJGP6CT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* call_tool: Executes the selected tool, removes the return_direct flag from the tool’s input (since the tool doesn’t need it), and then formats the tool’s response as a message to be added to the state."
      ],
      "metadata": {
        "id": "56AbO_Sm55H0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g8PavQ-Y6Ibo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Define the Workflow (Graph)"
      ],
      "metadata": {
        "id": "PwUwqxTb6Tee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the graph**\n",
        "\n",
        "We can now put it all together and define the graph!\n",
        "\n",
        "**MODIFICATION**\n",
        "\n",
        "We add a separate node for any tool call where return_direct=True. The reason this is needed is that after this node we want to end, while after other tool calls we want to go back to the LLM."
      ],
      "metadata": {
        "id": "VBthuO4dcZBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START  # Import necessary components from langgraph: END, StateGraph, and START\n",
        "\n",
        "workflow = StateGraph(AgentState)  # Create a new workflow (StateGraph) starting with the AgentState\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)  # Add a node called \"agent\" that runs the call_model function\n",
        "workflow.add_node(\"action\", call_tool)  # Add a node called \"action\" that runs the call_tool function\n",
        "workflow.add_node(\"final\", call_tool)  # Add a node called \"final\" that also runs the call_tool function\n",
        "\n",
        "workflow.add_edge(START, \"agent\")  # Connect the START point to the \"agent\" node, making \"agent\" the first step\n",
        "\n",
        "workflow.add_conditional_edges(  # Add conditional paths from the \"agent\" node based on a condition\n",
        "    \"agent\",  # The starting node for the condition\n",
        "    should_continue,  # The function that decides the path based on the condition\n",
        "    {\"continue\": \"action\", \"final\": \"final\", \"end\": END},  # The possible paths: to \"action\", \"final\", or END\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"action\", \"agent\")  # Connect the \"action\" node back to the \"agent\" node, creating a loop\n",
        "workflow.add_edge(\"final\", END)  # Connect the \"final\" node to END, meaning the workflow ends here\n",
        "\n",
        "app = workflow.compile()  # Compile the workflow into an application that can be executed\n"
      ],
      "metadata": {
        "id": "ByNkzaII6IYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "* StateGraph: Defines the flow of the conversation. Nodes represent different functions, and edges represent transitions between these nodes.\n",
        "* add_node: Adds the nodes (steps) to the graph:\n",
        "* agent: Calls the language model.\n",
        "* action: Executes a tool.\n",
        "* final: Executes a tool and then ends.\n",
        "* add_edge: Specifies the order in which nodes are called. For example, after the agent node, it checks whether to continue to the action node, go to the final node, or end the conversation.\n",
        "* compile: Finalizes the workflow into a runnable application."
      ],
      "metadata": {
        "id": "n8sk6U4G6bSK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gNr5Grx86IVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display  # Import the Image and display functions from IPython.display\n",
        "\n",
        "try:\n",
        "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))  # Try to display a visual representation of the workflow as a PNG image\n",
        "except Exception:  # If an error occurs (like missing dependencies), handle it here\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass  # Ignore the error and do nothing, allowing the program to continue running\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "2MkSwmZY5uR1",
        "outputId": "d23ee0b5-a0da-46a0-d8d4-fb11f17fdbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Run the Application"
      ],
      "metadata": {
        "id": "GuBlL7qy66gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage  # Import the HumanMessage class to create human-like messages\n",
        "\n",
        "# Create an input dictionary with a message asking about the weather in Pakistan\n",
        "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in Pakistan\")]}\n",
        "\n",
        "# Run the workflow using the input and stream the outputs\n",
        "for output in app.stream(inputs):\n",
        "\n",
        "    # Loop through each item (node's output) in the streamed output\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")  # Print the name of the node that produced the output\n",
        "        print(\"---\")  # Print a separator line for clarity\n",
        "        print(value)  # Print the actual output from the node\n",
        "\n",
        "    print(\"\\n---\\n\")  # Add a blank line with separators between different outputs\n"
      ],
      "metadata": {
        "id": "QI70dsud6ISb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446f04fc-8bd1-44f2-ff13-8b08f175ee86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_ab0ixNiPuhf6JoY9hgNqOVAw', 'function': {'arguments': '{\"query\":\"weather in Pakistan\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-4e079be0-6f29-42d1-b7f5-dd20ed9bab69-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in Pakistan'}, 'id': 'call_ab0ixNiPuhf6JoY9hgNqOVAw', 'type': 'tool_call'}])]}\n",
            "\n",
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-41c3f260fbee>:12: LangGraphDeprecationWarning: ToolInvocation is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
            "  action = ToolInvocation(tool=tool_name, tool_input=arguments)  # Create an action using 'ToolInvocation' with the tool's name and the modified arguments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'action':\n",
            "---\n",
            "{'messages': [ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Islamabad\\', \\'region\\': \\'Islamabad\\', \\'country\\': \\'Pakistan\\', \\'lat\\': 33.7, \\'lon\\': 73.17, \\'tz_id\\': \\'Asia/Karachi\\', \\'localtime_epoch\\': 1724923999, \\'localtime\\': \\'2024-08-29 14:33\\'}, \\'current\\': {\\'last_updated_epoch\\': 1724923800, \\'last_updated\\': \\'2024-08-29 14:30\\', \\'temp_c\\': 25.1, \\'temp_f\\': 77.2, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Patchy rain nearby\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/176.png\\', \\'code\\': 1063}, \\'wind_mph\\': 6.5, \\'wind_kph\\': 10.4, \\'wind_degree\\': 358, \\'wind_dir\\': \\'N\\', \\'pressure_mb\\': 1006.0, \\'pressure_in\\': 29.7, \\'precip_mm\\': 0.08, \\'precip_in\\': 0.0, \\'humidity\\': 80, \\'cloud\\': 89, \\'feelslike_c\\': 27.2, \\'feelslike_f\\': 81.0, \\'windchill_c\\': 25.1, \\'windchill_f\\': 77.2, \\'heatindex_c\\': 27.2, \\'heatindex_f\\': 81.0, \\'dewpoint_c\\': 21.5, \\'dewpoint_f\\': 70.7, \\'vis_km\\': 10.0, \\'vis_miles\\': 6.0, \\'uv\\': 6.0, \\'gust_mph\\': 7.5, \\'gust_kph\\': 12.0}}\"}]', name='tavily_search_results_json', tool_call_id='call_ab0ixNiPuhf6JoY9hgNqOVAw')]}\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'agent':\n",
            "---\n",
            "{'messages': [AIMessage(content='The current weather in Islamabad, Pakistan is as follows:\\n- Temperature: 25.1°C (77.2°F)\\n- Condition: Patchy rain nearby\\n- Wind: 10.4 km/h from the North\\n- Humidity: 80%\\n- Cloudiness: 89%\\n- Visibility: 10.0 km\\n\\nFor more detailed information, you can visit [Weather API](https://www.weatherapi.com/).', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-d67068ea-8c15-4de9-9661-a2ccf2fe201d-0')]}\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "* HumanMessage: The user’s input message to the chat agent.\n",
        "* app.stream(inputs): Runs the compiled graph with the provided input, simulating a conversation where the LLM and tools interact based on the workflow.\n",
        "* for output in app.stream(inputs): Loops through the outputs of each node, printing the result."
      ],
      "metadata": {
        "id": "X4nMy33M7DNy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbaS0UOD6IMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of the Project\n",
        "This project involves modifying a base chat agent executor, which is a system designed to interact with a language model (LLM) and tools in a loop. The modification allows the agent to dynamically decide whether to return the result of a tool call directly to the user without further processing, based on the specific context."
      ],
      "metadata": {
        "id": "cGhXayK_q4R2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1YGMVkpbq7V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose of the Project\n",
        "The purpose of this project is to create a more flexible and efficient chat agent that can:\n",
        "\n",
        "Directly return tool results to the user when the result is already sufficient as a final answer.\n",
        "Decide dynamically, based on the context, whether to return the result directly or to involve the LLM for further processing or summarization.\n",
        "This is particularly useful when some tools provide results that can sometimes be used as the final response. Instead of always processing the tool's output through the LLM, the agent can return the result directly if it's deemed appropriate, saving time and computational resources.\n",
        "\n",
        "This approach ensures that the chat agent can handle cases where direct responses are sufficient while still allowing for more complex processing when needed."
      ],
      "metadata": {
        "id": "CdpxrvzZq8nC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zowk9b92rCGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}